{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Research — Multi-Epoch Optimizer × Schedule Analysis\n",
    "\n",
    "This notebook explores results from extended multi-epoch sweeps to understand optimizer behavior, learning-rate schedule effects, and convergence trends over time. \n",
    "Data is loaded from the outputs generated in `optimizer_schedule_multi_epoch.csv` and `epoch_histories.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Create output directory for plots\n",
    "OUTPUT_DIR = Path(\"reports/scaling_validation_v3\")\n",
    "PLOT_DIR = OUTPUT_DIR / \"week5_plots\"\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Plot output directory: {PLOT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from existing experiments\n",
    "OUTPUT_DIR = Path(\"reports/scaling_validation_v3\")\n",
    "df = pd.read_csv(OUTPUT_DIR / \"optimizer_schedule_multi_epoch.csv\")\n",
    "with open(OUTPUT_DIR / \"epoch_histories.pkl\", \"rb\") as f:\n",
    "    histories = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(df)} experiment summaries and {len(histories)} history records.\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Accuracy vs Epochs (grouped by optimizer/schedule)\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create line plot for each optimizer-schedule combination\n",
    "for optimizer in df[\"optimizer\"].unique():\n",
    "    for schedule in df[\"schedule\"].unique():\n",
    "        subset = df[(df[\"optimizer\"] == optimizer) & \n",
    "                   (df[\"schedule\"] == schedule)].sort_values(\"epochs\")\n",
    "        if not subset.empty:\n",
    "            linestyle = '-' if optimizer == 'AdamW' else '--'\n",
    "            marker = 'o' if schedule == 'fixed' else 's' if schedule == 'step' else '^' if schedule == 'cosine' else 'D'\n",
    "            plt.plot(subset[\"epochs\"], subset[\"val_accuracy\"], \n",
    "                    linestyle=linestyle, marker=marker, markersize=8,\n",
    "                    label=f\"{optimizer} | {schedule}\", linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.xlabel(\"Epochs\", fontsize=14)\n",
    "plt.ylabel(\"Validation Accuracy\", fontsize=14)\n",
    "plt.title(\"Validation Accuracy vs Epochs by Optimizer and Schedule\", fontsize=16, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "acc_plot_path = PLOT_DIR / \"accuracy_vs_epochs.png\"\n",
    "plt.savefig(acc_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {acc_plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**  \n",
    "Notes and insights go here — describe convergence patterns, compare AdamW vs SGD, and mention how cosine/one-cycle schedules behaved across epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Loss vs Epochs (same grouping)\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create line plot for each optimizer-schedule combination\n",
    "for optimizer in df[\"optimizer\"].unique():\n",
    "    for schedule in df[\"schedule\"].unique():\n",
    "        subset = df[(df[\"optimizer\"] == optimizer) & \n",
    "                   (df[\"schedule\"] == schedule)].sort_values(\"epochs\")\n",
    "        if not subset.empty:\n",
    "            linestyle = '-' if optimizer == 'AdamW' else '--'\n",
    "            marker = 'o' if schedule == 'fixed' else 's' if schedule == 'step' else '^' if schedule == 'cosine' else 'D'\n",
    "            plt.plot(subset[\"epochs\"], subset[\"val_loss\"], \n",
    "                    linestyle=linestyle, marker=marker, markersize=8,\n",
    "                    label=f\"{optimizer} | {schedule}\", linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.xlabel(\"Epochs\", fontsize=14)\n",
    "plt.ylabel(\"Validation Loss\", fontsize=14)\n",
    "plt.title(\"Validation Loss vs Epochs by Optimizer and Schedule\", fontsize=16, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "loss_plot_path = PLOT_DIR / \"loss_vs_epochs.png\"\n",
    "plt.savefig(loss_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {loss_plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**  \n",
    "Notes and insights go here — analyze loss reduction patterns and note any schedules that achieve lower final loss values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: 2×2 metric grid (val/test accuracy/loss)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(\"Multi-Epoch Optimizer Comparison: 2×2 Metric Grid\", fontsize=18, fontweight='bold')\n",
    "\n",
    "# Focus on the longest training (12 epochs) for final performance comparison\n",
    "final_data = df[df[\"epochs\"] == df[\"epochs\"].max()]\n",
    "\n",
    "# Top-left: Validation accuracy\n",
    "ax = axes[0, 0]\n",
    "sns.barplot(data=final_data, x=\"schedule\", y=\"val_accuracy\", \n",
    "            hue=\"optimizer\", ax=ax, palette=\"Set2\")\n",
    "ax.set_title(\"Final Validation Accuracy\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Learning Rate Schedule\", fontsize=12)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "ax.legend(title=\"Optimizer\", fontsize=10)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Top-right: Validation loss\n",
    "ax = axes[0, 1]\n",
    "sns.barplot(data=final_data, x=\"schedule\", y=\"val_loss\", \n",
    "            hue=\"optimizer\", ax=ax, palette=\"Set2\")\n",
    "ax.set_title(\"Final Validation Loss\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Learning Rate Schedule\", fontsize=12)\n",
    "ax.set_ylabel(\"Loss\", fontsize=12)\n",
    "ax.legend(title=\"Optimizer\", fontsize=10)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Bottom-left: Training accuracy\n",
    "ax = axes[1, 0]\n",
    "sns.barplot(data=final_data, x=\"schedule\", y=\"train_accuracy\", \n",
    "            hue=\"optimizer\", ax=ax, palette=\"Set2\")\n",
    "ax.set_title(\"Final Training Accuracy\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Learning Rate Schedule\", fontsize=12)\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "ax.legend(title=\"Optimizer\", fontsize=10)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Bottom-right: ECE (Expected Calibration Error)\n",
    "ax = axes[1, 1]\n",
    "sns.barplot(data=final_data, x=\"schedule\", y=\"val_ece\", \n",
    "            hue=\"optimizer\", ax=ax, palette=\"Set2\")\n",
    "ax.set_title(\"Final Validation ECE (lower is better)\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Learning Rate Schedule\", fontsize=12)\n",
    "ax.set_ylabel(\"ECE\", fontsize=12)\n",
    "ax.legend(title=\"Optimizer\", fontsize=10)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save grid plot\n",
    "grid_plot_path = PLOT_DIR / \"metric_grid_2x2.png\"\n",
    "plt.savefig(grid_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {grid_plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**  \n",
    "Notes and insights go here — compare final performance across schedules and optimizers, noting any clear winners for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Learning curve overlays for each schedule\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(\"Learning Curve Overlays by Schedule\", fontsize=18, fontweight='bold')\n",
    "\n",
    "schedules = ['fixed', 'step', 'cosine', 'onecycle']\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "\n",
    "for i, schedule in enumerate(schedules):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    \n",
    "    # Plot learning curves for this schedule\n",
    "    for optimizer in df[\"optimizer\"].unique():\n",
    "        subset = df[(df[\"optimizer\"] == optimizer) & \n",
    "                   (df[\"schedule\"] == schedule)].sort_values(\"epochs\")\n",
    "        if not subset.empty:\n",
    "            color_idx = 0 if optimizer == 'SGD+Momentum' else 1\n",
    "            ax.plot(subset[\"epochs\"], subset[\"val_accuracy\"], \n",
    "                   marker='o', markersize=6, linewidth=2,\n",
    "                   label=optimizer, color=colors[color_idx], alpha=0.8)\n",
    "    \n",
    "    ax.set_title(f\"{schedule.title()} Schedule\", fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(\"Epochs\", fontsize=12)\n",
    "    ax.set_ylabel(\"Validation Accuracy\", fontsize=12)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save learning curves plot\n",
    "curves_plot_path = PLOT_DIR / \"learning_curve_overlays.png\"\n",
    "plt.savefig(curves_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {curves_plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**  \n",
    "Notes and insights go here — analyze how each schedule affects learning dynamics and convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Detailed epoch-by-epoch curves from history data\n",
    "if histories:\n",
    "    # Select representative configurations for detailed analysis\n",
    "    key_configs = [\n",
    "        \"TinyCNN|SGD+Momentum|fixed|epochs=12\",\n",
    "        \"TinyCNN|SGD+Momentum|cosine|epochs=12\", \n",
    "        \"TinyCNN|AdamW|fixed|epochs=12\",\n",
    "        \"TinyCNN|AdamW|cosine|epochs=12\"\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(\"Detailed Epoch-by-Epoch Training Curves\", fontsize=18, fontweight='bold')\n",
    "    \n",
    "    plot_colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "    \n",
    "    for i, (config, color) in enumerate(zip(key_configs, plot_colors)):\n",
    "        if config in histories:\n",
    "            history = histories[config]\n",
    "            epochs = list(range(1, len(history) + 1))\n",
    "            val_acc = [h[\"val_accuracy\"] for h in history]\n",
    "            val_loss = [h[\"val_loss\"] for h in history]\n",
    "            train_acc = [h[\"train_accuracy\"] for h in history]\n",
    "            \n",
    "            row = i // 2\n",
    "            col = i % 2\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            # Plot validation accuracy\n",
    "            ax.plot(epochs, val_acc, color=color, marker='o', linewidth=2, \n",
    "                   markersize=6, label='Val Accuracy', alpha=0.8)\n",
    "            \n",
    "            # Plot training accuracy\n",
    "            ax.plot(epochs, train_acc, color=color, marker='s', linewidth=2, \n",
    "                   linestyle='--', markersize=6, label='Train Accuracy', alpha=0.7)\n",
    "            \n",
    "            # Extract optimizer and schedule from config\n",
    "            parts = config.split('|')\n",
    "            optimizer = parts[1]\n",
    "            schedule = parts[2]\n",
    "            \n",
    "            ax.set_title(f\"{optimizer} | {schedule}\", fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel(\"Epoch\", fontsize=12)\n",
    "            ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save detailed curves\n",
    "    detailed_path = PLOT_DIR / \"detailed_epoch_curves.png\"\n",
    "    plt.savefig(detailed_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {detailed_path}\")\n",
    "    plt.show()\nelse:\n",
    "    print(\"No history data available for detailed curves.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**  \n",
    "Notes and insights go here — analyze training vs validation dynamics and identify any overfitting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics and best performers\n",
    "print(\"=== Multi-Epoch Results Summary ===\")\n",
    "print(f\"Total experiments: {len(df)}\")\n",
    "print(f\"Epochs tested: {sorted(df['epochs'].unique())}\")\n",
    "print(f\"Optimizers: {list(df['optimizer'].unique())}\")\n",
    "print(f\"Schedules: {list(df['schedule'].unique())}\")\n",
    "\n",
    "print(\"\\n=== Best Performing Configurations ===\")\n",
    "print(\"\\nBy Validation Accuracy:\")\n",
    "best_acc = df.loc[df['val_accuracy'].idxmax()]\n",
    "print(f\"  {best_acc['config_id']}: {best_acc['val_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nBy Validation Loss (lowest):\")\n",
    "best_loss = df.loc[df['val_loss'].idxmin()]\n",
    "print(f\"  {best_loss['config_id']}: {best_loss['val_loss']:.4f}\")\n",
    "\n",
    "print(\"\\nBy ECE (lowest):\")\n",
    "best_ece = df.loc[df['val_ece'].idxmin()]\n",
    "print(f\"  {best_ece['config_id']}: {best_ece['val_ece']:.4f}\")\n",
    "\n",
    "# Performance by optimizer (averaged across schedules and epochs)\n",
    "print(\"\\n=== Performance by Optimizer (Averaged) ===\")\n",
    "optimizer_stats = df.groupby('optimizer').agg({\n",
    "    'val_accuracy': ['mean', 'std'],\n",
    "    'val_loss': ['mean', 'std'],\n",
    "    'val_ece': ['mean', 'std']\n",
    "}).round(4)\n",
    "print(optimizer_stats)\n",
    "\n",
    "# Performance by schedule (averaged across optimizers and epochs)\n",
    "print(\"\\n=== Performance by Schedule (Averaged) ===\")\n",
    "schedule_stats = df.groupby('schedule').agg({\n",
    "    'val_accuracy': ['mean', 'std'],\n",
    "    'val_loss': ['mean', 'std'],\n",
    "    'val_ece': ['mean', 'std']\n",
    "}).round(4)\n",
    "print(schedule_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Multi-epoch sweeps (3–12 epochs) reveal optimizer stability and schedule trends.\n",
    "- Cosine and One-Cycle schedules generally improve convergence speed and validation loss.\n",
    "- Plots are exported to `week5_plots/` for embedding in Notion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
